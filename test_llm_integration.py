"""
ç®€åŒ–çš„LangGraph + LLMæ¼”ç¤ºç¨‹åº
ç”¨äºæµ‹è¯•DeepSeeké›†æˆå’ŒLangGraphå·¥ä½œæµ
"""

import os
import asyncio
import math
from typing import Dict, Any, List
from llm_config import DeepSeekClient, agent_prompts, check_llm_status
from environment import Environment

def test_deepseek_connection():
    """æµ‹è¯•DeepSeekè¿æ¥"""
    print("ğŸ” æµ‹è¯•DeepSeek LLMè¿æ¥...")
    
    client = DeepSeekClient()
    if not client.is_available():
        print("âš ï¸ DeepSeek APIå¯†é’¥æœªé…ç½®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæµ‹è¯•")
        return False
    
    # æµ‹è¯•APIè°ƒç”¨
    test_messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæµ‹è¯•åŠ©æ‰‹"},
        {"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç»è‡ªå·±"}
    ]
    
    response = client.chat_completion(test_messages)
    if response:
        print(f"âœ… DeepSeekè¿æ¥æˆåŠŸ!")
        print(f"ğŸ“ å›åº”: {response[:100]}...")
        return True
    else:
        print("âŒ DeepSeekè¿æ¥å¤±è´¥")
        return False

async def test_llm_customer_decision():
    """æµ‹è¯•LLMå®¢æˆ·å†³ç­–"""
    print("\nğŸ§ª æµ‹è¯•LLMå®¢æˆ·å†³ç­–")
    
    client = DeepSeekClient()
    
    # æ¨¡æ‹Ÿç¯å¢ƒçŠ¶æ€
    temp = 42.5
    is_meal_time = True
    hour = 12
    avg_rating = 4.2
    
    system_prompt = agent_prompts.CUSTOMER_SYSTEM
    user_prompt = f"""
å½“å‰ç¯å¢ƒ:
- æ¸©åº¦: {temp:.1f}Â°C (æç«¯é«˜æ¸©!)
- æ—¶é—´: {hour}ç‚¹ (åˆé¤æ—¶é—´)
- æ˜¯å¦ç”¨é¤æ—¶é—´: {is_meal_time}
- æˆ‘çš„å†å²è¯„åˆ†å‡å€¼: {avg_rating:.1f}

è€ƒè™‘åˆ°æç«¯é«˜æ¸©å¯èƒ½å½±å“éª‘æ‰‹å¥åº·å’Œé…é€è´¨é‡ï¼Œè¯·å†³å®šæ˜¯å¦ç‚¹å¤–å–ï¼Ÿ

è¯·å›ç­” {{"order": true/false, "concern_level": "high/medium/low", "reasoning": "å†³ç­–ç†ç”±"}}
"""
    
    if client.is_available():
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        response = client.chat_completion(messages)
        if response:
            print(f"ğŸ¤– LLMå®¢æˆ·åˆ†æ:")
            print(f"   {response}")
        else:
            print("âŒ LLMå†³ç­–å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™å†³ç­–")
    else:
        print("âš ï¸ ä½¿ç”¨è§„åˆ™åŸºç¡€å†³ç­–: é«˜æ¸©+ç”¨é¤æ—¶é—´ = 50%æ¦‚ç‡ä¸‹å•")

async def test_llm_rider_decision():
    """æµ‹è¯•LLMéª‘æ‰‹å†³ç­–"""
    print("\nğŸ§ª æµ‹è¯•LLMéª‘æ‰‹å†³ç­–")
    
    client = DeepSeekClient()
    
    # æ¨¡æ‹Ÿéª‘æ‰‹çŠ¶æ€
    health = 3.5
    money = 850
    happiness = 2.8
    temp = 45.2
    order_count = 3
    
    system_prompt = agent_prompts.RIDER_SYSTEM.format(
        health=health, money=money, happiness=happiness
    )
    
    user_prompt = f"""
å½“å‰å·¥ä½œç¯å¢ƒ:
- æ¸©åº¦: {temp:.1f}Â°C (å±é™©é«˜æ¸©!)
- å¯æ¥è®¢å•æ•°: {order_count}
- æˆ‘çš„å¥åº·çŠ¶å†µ: {health:.1f}/10 (è¾ƒå·®)
- å½“å‰èµ„é‡‘: {money:.0f}å…ƒ

åœ¨å¥åº·ä¸ä½³çš„æƒ…å†µä¸‹ï¼Œé¢å¯¹å±é™©é«˜æ¸©å’Œæ”¶å…¥éœ€æ±‚ï¼Œè¯·é€‰æ‹©è¡ŒåŠ¨:
1. deliver - æ¥å•é…é€ (æ”¶å…¥ä½†æŸå®³å¥åº·)
2. rest - ä¼‘æ¯æ¢å¤ (æ¢å¤å¥åº·ä½†æ— æ”¶å…¥)
3. complain - æŠ•è¯‰å·¥ä½œæ¡ä»¶ (è¡¨è¾¾ä¸æ»¡)

è¯·å›ç­” {{"action": "deliver/rest/complain", "risk_level": "high", "reasoning": "å†³ç­–ç†ç”±"}}
"""
    
    if client.is_available():
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        response = client.chat_completion(messages)
        if response:
            print(f"ğŸ¤– LLMéª‘æ‰‹åˆ†æ:")
            print(f"   {response}")
        else:
            print("âŒ LLMå†³ç­–å¤±è´¥")
    else:
        print("âš ï¸ ä½¿ç”¨è§„åˆ™åŸºç¡€å†³ç­–: å¥åº·å·®+æç«¯é«˜æ¸© = ä¼˜å…ˆä¼‘æ¯")

async def test_llm_government_policy():
    """æµ‹è¯•LLMæ”¿åºœæ”¿ç­–å†³ç­–"""
    print("\nğŸ§ª æµ‹è¯•LLMæ”¿åºœæ”¿ç­–å†³ç­–")
    
    client = DeepSeekClient()
    
    # æ¨¡æ‹Ÿç¤¾ä¼šçŠ¶å†µ
    temp = 43.8
    complaints = 7
    unhealthy_riders = 5
    shelter_rate = 0.3
    budget = 5000
    
    user_prompt = f"""
å½“å‰ç¤¾ä¼šçŠ¶å†µç´§æ€¥ï¼
- æ¸©åº¦: {temp:.1f}Â°C (æç«¯é«˜æ¸©é¢„è­¦)
- éª‘æ‰‹æŠ•è¯‰æ•°: {complaints} (è¾ƒå¤š)
- å¥åº·çŠ¶å†µä¸ä½³éª‘æ‰‹æ•°: {unhealthy_riders}
- çº³å‡‰ç‚¹è¦†ç›–ç‡: {shelter_rate:.1f} (è¦†ç›–ä¸è¶³)
- æ”¿åºœé¢„ç®—: {budget:.0f}å…ƒ

ä½œä¸ºè´Ÿè´£åŠ³åŠ¨è€…ä¿æŠ¤çš„æ”¿åºœå®˜å‘˜ï¼Œè¯·åˆ¶å®šç´§æ€¥æ”¿ç­–æªæ–½ï¼š

1. é«˜æ¸©è¡¥è´´å»ºè®®ï¼š
   - æ¸©åº¦>38Â°C: 30-50å…ƒ/äºº
   - æ¸©åº¦>42Â°C: 50-100å…ƒ/äºº

2. åŸºç¡€è®¾æ–½å»ºè®¾ï¼š
   - å¢è®¾çº³å‡‰ç‚¹æˆæœ¬: 1000å…ƒ/ä¸ª
   - æŠ•è¯‰å¤šä¸”è¦†ç›–ç‡ä½æ—¶ä¼˜å…ˆå»ºè®¾

è¯·å›ç­” {{"subsidy_amount": æ•°å­—, "build_shelter": true/false, "urgency": "high", "reasoning": "æ”¿ç­–ç†ç”±"}}
"""
    
    if client.is_available():
        messages = [
            {"role": "system", "content": agent_prompts.GOVERNMENT_SYSTEM},
            {"role": "user", "content": user_prompt}
        ]
        
        response = client.chat_completion(messages)
        if response:
            print(f"ğŸ¤– LLMæ”¿åºœåˆ†æ:")
            print(f"   {response}")
        else:
            print("âŒ LLMå†³ç­–å¤±è´¥")
    else:
        print("âš ï¸ ä½¿ç”¨è§„åˆ™åŸºç¡€å†³ç­–: æç«¯é«˜æ¸©+å¤šæŠ•è¯‰ = è¡¥è´´70å…ƒ+å»ºçº³å‡‰ç‚¹")

def simulate_simple_workflow():
    """æ¨¡æ‹Ÿç®€å•çš„å·¥ä½œæµ"""
    print("\nğŸ”„ æ¨¡æ‹Ÿç®€åŒ–å·¥ä½œæµ")
    
    # ç¯å¢ƒçŠ¶æ€
    env = Environment()
    env.current_hour = 12
    # ç”Ÿæˆæ–°çš„é«˜æ¸©æ›²çº¿æ¥æ¨¡æ‹Ÿæç«¯æƒ…å†µ
    env.temperature_curve = [35 + 10 * math.sin((h - 6) * math.pi / 12) + 5 for h in range(24)]
    env_state = env.get_environment_state()
    
    print(f"ğŸŒ¡ï¸ ç¯å¢ƒ: {env_state['temperature']:.1f}Â°C, {env_state['hour']}ç‚¹")
    
    # 1. å®¢æˆ·è§‚å¯Ÿå†³ç­–
    print("ğŸ‘¤ å®¢æˆ·è§‚å¯Ÿç¯å¢ƒ...")
    if env_state['is_meal_time'] and env_state['temperature'] > 40:
        print("   ğŸ¤” åˆé¤æ—¶é—´ä½†é«˜æ¸©ï¼Œæ‹…å¿ƒéª‘æ‰‹å¥åº·")
        order_decision = "è€ƒè™‘åˆ°é«˜æ¸©ï¼Œæš‚ä¸ä¸‹å•"
    else:
        order_decision = "æ­£å¸¸ä¸‹å•"
    print(f"   ğŸ“ å†³ç­–: {order_decision}")
    
    # 2. éª‘æ‰‹è¯„ä¼°é£é™©
    print("ğŸš´ éª‘æ‰‹è¯„ä¼°å·¥ä½œç¯å¢ƒ...")
    rider_health = 4.2
    if env_state['temperature'] > 40 and rider_health < 5:
        action = "å¥åº·ä¸ä½³ï¼Œé€‰æ‹©ä¼‘æ¯"
    else:
        action = "æ¥å•é…é€"
    print(f"   ğŸ’­ å¥åº·: {rider_health}/10")
    print(f"   ğŸ“ å†³ç­–: {action}")
    
    # 3. æ”¿åºœç›‘æµ‹æƒ…å†µ
    print("ğŸ›ï¸ æ”¿åºœç›‘æµ‹ç¤¾ä¼šçŠ¶å†µ...")
    if env_state['temperature'] > 40:
        policy = f"å¯åŠ¨é«˜æ¸©é¢„è­¦ï¼Œå‘æ”¾è¡¥è´´50å…ƒ/äºº"
    else:
        policy = "æ­£å¸¸ç›‘æµ‹"
    print(f"   ğŸ“ æ”¿ç­–: {policy}")
    
    # 4. å¹³å°è°ƒæ•´ç­–ç•¥
    print("ğŸ’¼ å¹³å°åˆ†æè¿è¥æ•°æ®...")
    platform_action = "è€ƒè™‘åˆ°é«˜æ¸©å½±å“ï¼Œæš‚æ—¶æé«˜éª‘æ‰‹åˆ†æˆæ¯”ä¾‹"
    print(f"   ğŸ“ å†³ç­–: {platform_action}")

async def main():
    """ä¸»æµ‹è¯•ç¨‹åº"""
    print("ğŸ¤– LangGraph + DeepSeek LLM é›†æˆæµ‹è¯•")
    print("="*50)
    
    # æ£€æŸ¥LLMçŠ¶æ€
    check_llm_status()
    
    # æµ‹è¯•DeepSeekè¿æ¥
    llm_available = test_deepseek_connection()
    
    if llm_available:
        print("\nğŸ§  æµ‹è¯•LLMæ™ºèƒ½å†³ç­–èƒ½åŠ›")
        await test_llm_customer_decision()
        await test_llm_rider_decision()
        await test_llm_government_policy()
    else:
        print("\nâš ï¸ LLMä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨è§„åˆ™åŸºç¡€å†³ç­–")
    
    # æ¨¡æ‹Ÿå·¥ä½œæµ
    simulate_simple_workflow()
    
    print("\n" + "="*50)
    print("âœ… æµ‹è¯•å®Œæˆ!")
    
    if llm_available:
        print("ğŸ¯ ç³»ç»Ÿå·²å‡†å¤‡å¥½è¿è¡Œå®Œæ•´LangGraph+LLMä»¿çœŸ")
        print("ğŸ’¡ è¿è¡Œ python langgraph_simulation.py å¼€å§‹å®Œæ•´ä»¿çœŸ")
    else:
        print("ğŸ’¡ é…ç½®DeepSeek APIå¯†é’¥åå¯ä½¿ç”¨LLMæ™ºèƒ½å†³ç­–")
        print("ğŸ”§ åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®DEEPSEEK_API_KEY")

if __name__ == "__main__":
    asyncio.run(main())
